---
title: 在服务器使用 Ollama 本地部署 DeepSeek-r1:8b 模型
date: 2025-02-05 01:10:00
categories: ["默认分类"]
tags: []
---

> 本文记录在实验室服务器上部署本地大语言模型 `deepseek-r1:8b` 的完整过程。通过 `Ollama` 实现模型拉取和运行，限制 API 只能本地访问，同时集成 `OpenWebUI` 作为可视化前端，构建一个私密、安全、稳定的本地大模型平台。

展示：
![2025-04-09T17:10:17.png][1]
点击访问 [Deepseek-r1:8b][2]
## 背景介绍

实验室近期需要部署一个本地的大语言模型来进行代码生成类实验。考虑到以下几点：

- 对话能力强，尤其偏向代码编写
- 模型能运行在消费级或普通科研服务器（如 1～2 张 3090）
- 安全性高，不暴露 API 给公网
- 可通过 Web 界面直接访问模型

最终选择使用：
- 模型：[`deepseek-coder:7b`](https://huggingface.co/deepseek-ai/deepseek-coder-7b-base)
- 后端：[`Ollama`](https://ollama.com/)
- 前端：[`OpenWebUI`](https://github.com/open-webui/open-webui)

---

## Ollama 安装与部署

### 1. 安装 Ollama

Ollama 提供了简洁的 CLI 工具用于运行 LLM，本地下载即可运行。

```bash
# Ubuntu / Debian
curl -fsSL https://ollama.com/install.sh | sh

# 验证安装成功
ollama --version
```
### 2. 拉取模型
```bash
ollama pull deepseek-r1:8b
```

### 安全策略
为了保证服务器安全，我们不对外暴露 Ollama API 接口，仅开放本地 localhost:11434，这样即使服务器有公网 IP，模型服务也不会被外部访问。

### 集成 OpenWebUI 前端界面
为了方便交互使用，部署开源的可视化前端：OpenWebUI，支持直接连接 Ollama。
1. Docker 启动
官网命令：
```bash
docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
```
2. 登录使用 
实验室服务器打开 http://localhost:3000 即可访问 OpenWebUI，首次使用需注册用户账号，之后即可登录使用。
3. 域名访问
使用 Caddy 配置域名反向代理到 OpenWebUI 端口，实现通过域名访问，方便大家使用。Caddy 自动支持 HTTPS，基于 Go 编写，是一个非常好用的轻量网站服务。
```bash
deepseek.example.com {
    reverse_proxy localhost:3000
}

```


![2025-04-09T16:55:22.png][3]


  [1]: https:///images/typecho//2025/04/2731588404.png
  [2]: https://deepseek.seeu.eu.org/
  [3]: https:///images/typecho//2025/04/2477342134.png
