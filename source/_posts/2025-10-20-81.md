---
title: 🔢 向量化：让机器更 “懂”人类文字
date: 2025-10-20 17:37:00
categories: ["默认分类"]
tags: []
---

# 🔢 向量化：让机器更 “懂”人类文字

科研人在检索论文的时候一定有过这种感觉——研究方向的关键词检索，往往检索不到所有可能启发我们的论文。因为技术是通用的，完全可能有别的领域的论文所做的工作和我们领域的类似，但由于我们搜索的仅仅是我们领域的关键词，于是检索不到，错失了可能的启发。

我的研究场景是 `MobileCrowdsensing` ，其实 `Crowdsourcing` 也是类似的场景，但我在检索的时候往往要分别检索这两个关键词来寻找论文，这便是单纯文本匹配检索的弊端。

还有我们的工作可能包含分布式的无人机行动规划，其实这种规划类似于自动驾驶的规划，完全可以从中参照，因此简单的检索引擎也会让我错失一些启发性的优质文章。

## 向量化是啥


“我喜欢吃苹果。”
“我爱吃水果。”

这两句话**意思接近**，它们在**高维空间的“距离”也很近**。
而“我在写代码”那条向量，就离它们老远。

计算机通过这种方式“感受”语义上的相似程度，
有点像人脑中的联想机制。

--- 

## 为什么要向量化

以前的搜索是关键词匹配——你搜“程序员”，它找不到“开发者”。
但向量化不看字面，而是看语义。

这就是为什么在 RAG（检索增强生成）系统里，
我们要先把所有文档都向量化、存进数据库。
当用户提问时，也把问题向量化，再找“最近”的那几段内容给模型看。
模型就能在有上下文的前提下回答问题。

一句话：向量化让模型先理解，再回答。

--- 

## 向量是怎么来的

靠的是 **Embedding 模型**。
比如 OpenAI 的 text-embedding-3、阿里百炼的 text-embedding-v2、
还有开源的 bge-large、m3e 等。

这些模型经过大规模语料训练，能把语义转成数字。
比如一句话会被转成一个 1024 维的数组 [0.12, -0.03, 0.85, …]，
我们看不懂，但在机器眼里，这就是“意思”。

--- 

## 写在最后

向量化让机器能够进行“语义检索”，更好地“理解”人类语言。
想想向量化有点像人类的思考，我们的脑海中 巴士 和 公交 映射出来的画面大概率都是一辆 🚌，于是这两个词的向量距离就相近，当有人在公交站和我们说 巴士怎么还不来 的时候，我们理所当然的能够理解是在说我们等的车怎么还不来。
