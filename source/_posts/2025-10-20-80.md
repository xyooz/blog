---
title: RAG 与 模型幻觉
date: 2025-10-20 17:21:22
categories: ["默认分类"]
tags: []
---

# 🤯 RAG 与大模型幻觉：让模型“说真话”的秘密武器

如果你经常和 ChatGPT、通义千问之类的大模型打交道，你大概见识过它一本正经地胡说八道的样子。
比如你问它：“我上周的论文标题是什么？”
它会一本正经地回答：“《跨模态差分隐私下的自适应生成网络研究》。”
听起来很专业，但——根本不存在。

这就是所谓的 **幻觉**。

--- 

## 幻觉从哪来？

简单说，语言模型的核心任务是“预测下一个词”。它并不真正理解事实，只是在大量语料中学习“词语搭配的概率”。当你问它问题时，它会根据语义猜一个“看起来合理的答案”，
**但并不保证那是真的**。

--- 

## RAG 是怎么帮忙的？

RAG（Retrieval-Augmented Generation）可以理解为模型的外置事实引擎。
它不再“凭记忆瞎猜”，而是先去查资料。

当你提问时，系统会先用检索模型（embedding + 向量库）找出与问题最相关的知识片段，再把这些片段拼进 prompt 一起交给模型生成。

--- 

## 为什么 RAG 不能完全消灭幻觉？

RAG 能显著降低幻觉，但无法完全消灭。原因有三：
1. **检索不准**：如果向量库召回了错误片段，模型仍然可能胡说。
2. **引用不严谨**：模型有时会“二次加工”资料，把原话改得面目全非。
3. **知识缺失**：如果知识库本身没有这条信息，RAG 也帮不了忙。

换句话说，RAG 不是防幻觉的“护身符”，而是一个让模型“有依据可查”的事实来源。

--- 

## 工程实践：如何让 RAG 更可靠？

实践中想让 RAG 真正降低幻觉，需要几条关键策略：
- **知识分段合理**：段太长检索不准，太短语义碎。
- **embedding 模型质量**高：保证语义相似度匹配准确。
- 检索 + rerank **混合召回**：向量 + BM25 双保险。
- **Prompt** 结构清晰：提示模型“请仅根据以下内容回答”。
- **引用验证**：生成结果后可做语义比对，检测回答是否出自检索文本。

有些企业项目甚至会在 RAG 结果上再套一层 “Fact Check 模块”，
自动比对生成内容与原文相似度，**进一步过滤幻觉**。

---

## 写在最后

RAG 其实是利用知识库来为模型提供上下文，以给出相对可靠的答案。但是在过程中，由于**回答是由AI给出**，即便有了相关的上下文，仍然可能给出不靠谱的答案；另外知识库中检索出来的知识精度（**召回率**）也是一大影响，只有准确的检索才能引导模型给出靠谱的答案，因此可以从**embedding模型、输出验证等**下手，提高回答精度。
